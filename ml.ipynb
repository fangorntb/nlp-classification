{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvryNQ5PV+7fGpJpmG97U6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fangorntreabeard/nlp-classification/blob/main/ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7kBf-Rw-s8Ad"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = output[-1, :, :]\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Предобработка текста и создание словаря\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "    return tokens\n",
        "\n",
        "def create_vocab(texts):\n",
        "    vocab = set()\n",
        "    for text in texts:\n",
        "        tokens = preprocess_text(text)\n",
        "        vocab.update(tokens)\n",
        "    word_to_idx = {word: idx+1 for idx, word in enumerate(vocab)}\n",
        "    return word_to_idx\n",
        "\n",
        "# Обучение модели\n",
        "def train_model(model, train_data, train_labels, num_epochs, batch_size):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = len(train_data) // batch_size\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "\n",
        "            inputs = train_data[start_idx:end_idx]\n",
        "            labels = train_labels[start_idx:end_idx]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "# Пример использования\n",
        "# Подготовка обучающих данных\n",
        "train_texts = [\n",
        "    \"Этот фильм просто прекрасен!\",\n",
        "    \"Эта книга ужасна, не советую читать.\",\n",
        "    \"Сегодня погода хорошая.\"\n",
        "]\n",
        "train_labels = torch.tensor([0, 1, 0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка обучающих данных\n",
        "train_texts = [\n",
        "    \"Этот фильм просто прекрасен!\",\n",
        "    \"Эта книга ужасна, не советую читать.\",\n",
        "    \"Сегодня погода хорошая.\"\n",
        "]\n",
        "train_labels = torch.tensor([0, 1, 0])  # 0 - положительный, 1 - отрицательный\n",
        "\n",
        "# Создание словаря\n",
        "vocab = create_vocab(train_texts)\n",
        "\n",
        "# Преобразование текста в числовые последовательности\n",
        "train_data = []\n",
        "for text in train_texts:\n",
        "    tokens = preprocess_text(text)\n",
        "    sequence = [vocab[word] for word in tokens if word in vocab]\n",
        "    train_data.append(sequence)\n",
        "\n",
        "# Дополнение последовательностей до одинаковой длины\n",
        "max_len = max(len(sequence) for sequence in train_data)\n",
        "train_data = [sequence + [0] * (max_len - len(sequence)) for sequence in train_data]\n",
        "\n",
        "# Преобразование данных в тензоры PyTorch\n",
        "train_data = torch.tensor(train_data)\n",
        "train_labels = train_labels.long()\n",
        "\n",
        "# Определение гиперпараметров\n",
        "input_dim = len(vocab) + 1  # +1 для пустого токена\n",
        "hidden_dim = 100\n",
        "output_dim = 2  # два класса: положительный и отрицательный\n",
        "num_epochs = 10\n",
        "batch_size = 1\n",
        "\n",
        "# Создание и обучение модели\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "train_model(model, train_data, train_labels, 11, 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KOlQuTEueBg",
        "outputId": "58ccd0de-6a23-45f5-cd27-0c777a273ebf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11, Loss: 0.0\n",
            "Epoch 2/11, Loss: 0.0\n",
            "Epoch 3/11, Loss: 0.0\n",
            "Epoch 4/11, Loss: 0.0\n",
            "Epoch 5/11, Loss: 0.0\n",
            "Epoch 6/11, Loss: 0.0\n",
            "Epoch 7/11, Loss: 0.0\n",
            "Epoch 8/11, Loss: 0.0\n",
            "Epoch 9/11, Loss: 0.0\n",
            "Epoch 10/11, Loss: 0.0\n",
            "Epoch 11/11, Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования обученной модели для предсказания\n",
        "test_texts = [\n",
        "    \"Этот фильм был отличным!\",\n",
        "    \"Мне не понравилась эта книга.\",\n",
        "    \"Погода сегодня ужасная.\"\n",
        "]\n",
        "\n",
        "# Предобработка и преобразование тестовых текстов\n",
        "test_data = []\n",
        "for text in test_texts:\n",
        "    tokens = preprocess_text(text)\n",
        "    sequence = [vocab[word] for word in tokens if word in vocab]\n",
        "    test_data.append(sequence)\n",
        "\n",
        "# Дополнение последовательностей до одинаковой длины\n",
        "test_data = [sequence + [0] * (max_len - len(sequence)) for sequence in test_data]\n",
        "\n",
        "# Преобразование данных в тензоры PyTorch\n",
        "test_data = torch.tensor(test_data)\n",
        "\n",
        "# Вызов предсказания\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions = model(test_data)\n",
        "\n",
        "# Преобразование выходов модели в метки классов\n",
        "predicted_labels = torch.argmax(predictions, dim=1).tolist()\n",
        "\n",
        "# Вывод результатов\n",
        "for text, label in zip(test_texts, predicted_labels):\n",
        "    if label == 0:\n",
        "        print(f\"Текст: {text}\\nКлассификация: Положительный\\n\")\n",
        "    else:\n",
        "        print(f\"Текст: {text}\\nКлассификация: Отрицательный\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCGUAzRYu9KT",
        "outputId": "4ebee60f-b308-4c51-dcc1-d5226ec356bf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст: Этот фильм был отличным!\n",
            "Классификация: Положительный\n",
            "\n",
            "Текст: Мне не понравилась эта книга.\n",
            "Классификация: Положительный\n",
            "\n",
            "Текст: Погода сегодня ужасная.\n",
            "Классификация: Положительный\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[lookups] && python -m spacy download ru_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C5lEsjGt_h_",
        "outputId": "b2f2488b-96f0-417e-bea5-70950e4a5d6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy[lookups] in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[lookups]) (3.3.0)\n",
            "Collecting spacy-lookups-data<1.1.0,>=1.0.3 (from spacy[lookups])\n",
            "  Downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy[lookups]) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[lookups]) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[lookups]) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy[lookups]) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy[lookups]) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy[lookups]) (2.1.3)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.3\n",
            "2023-06-26 17:20:05.276862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.5.0/ru_core_news_sm-3.5.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.5.0) (3.5.3)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.5.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.1.3)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=407b1e1455172022b8da45b6a281660d87f7e804bd587aea653474ecb628077c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "###############################РАБОЧАЯ МОДЕЛЬ###################################\n",
        "################################################################################\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = 'bert-base-multilingual-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "train_texts = [\n",
        "    \"Любовь - это сила, которая двигает мир вперед.\",\n",
        "    \"Семья - это источник непреходящей поддержки и любви.\",\n",
        "    \"Дружба - это связь, которая делает нашу жизнь яркой и значимой.\",\n",
        "    \"Деньги - это средство, которое помогает обеспечить нашу жизнь и достижение целей.\",\n",
        "    \"Любовь может исцелить и преобразить даже самые тяжелые ситуации.\",\n",
        "    \"Семья - это те люди, которые всегда будут рядом, независимо от всего.\",\n",
        "    \"Дружба - это взаимопонимание, поддержка и смех на протяжении всей жизни.\",\n",
        "    \"Деньги важны, но настоящее богатство заключается в людях, которых мы имеем в своей жизни.\",\n",
        "]\n",
        "\n",
        "test_texts = [\n",
        "    \"Хотелось бы найти девушку.\",\n",
        "    \"Друг -- человек, на которого всегда можно положиться\",\n",
        "    \"Финансовые вопросы стали серьезной проблемой для многих людей.\",\n",
        "]\n",
        "\n",
        "train_labels = [\n",
        "    [1, 1, 0, 0],  # Любовь, Семья, Дружба, Деньги\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 1],\n",
        "    [0, 0, 0, 1],\n",
        "]\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for text in train_texts:\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    encoded_inputs = tokenizer.encode_plus(\n",
        "        preprocessed_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids.append(encoded_inputs['input_ids'])\n",
        "    attention_masks.append(encoded_inputs['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "# Обучение модели\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    loss = criterion(outputs.logits, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# Вызов предсказания\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_input_ids = []\n",
        "    test_attention_masks = []\n",
        "    for text in test_texts:\n",
        "        preprocessed_text = preprocess_text(text)\n",
        "        encoded_inputs = tokenizer.encode_plus(\n",
        "            preprocessed_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        test_input_ids.append(encoded_inputs['input_ids'])\n",
        "        test_attention_masks.append(encoded_inputs['attention_mask'])\n",
        "\n",
        "    test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "    test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "\n",
        "    outputs = model(test_input_ids, attention_mask=test_attention_masks)\n",
        "    predicted_labels = torch.sigmoid(outputs.logits)\n",
        "\n",
        "# Вывод результатов\n",
        "label_map = {0: \"любовь\", 1: \"семья\", 2: \"дружба\", 3: \"деньги\"}\n",
        "for text, labels in zip(test_texts, predicted_labels.tolist()):\n",
        "    predicted_values = [[label_map[i], label] for i, label in enumerate(labels)]\n",
        "    print(f\"Текст: {text}\")\n",
        "    print(f\"Классификация: {predicted_values}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmP9GSYtwsd4",
        "outputId": "35ed139b-6543-4f13-e490-3363218bd1b6"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст: Хотелось бы найти девушку.\n",
            "Классификация: [['любовь', 0.517228901386261], ['семья', 0.39512184262275696], ['дружба', 0.24947544932365417], ['деньги', 0.5338225364685059]]\n",
            "\n",
            "Текст: Друг -- человек, на которого всегда можно положиться\n",
            "Классификация: [['любовь', 0.5091134309768677], ['семья', 0.4085809290409088], ['дружба', 0.25483426451683044], ['деньги', 0.5077824592590332]]\n",
            "\n",
            "Текст: Финансовые вопросы стали серьезной проблемой для многих людей.\n",
            "Классификация: [['любовь', 0.47441405057907104], ['семья', 0.357241690158844], ['дружба', 0.2906544804573059], ['деньги', 0.5126874446868896]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"path/to/save/directory\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "g0Zkc8L8K32D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpgpuh4ewu2G",
        "outputId": "146db644-f7db-4467-89c0-9f140e3031c7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "dataset = [\n",
        "    {'text': 'Данные 1', 'values': ['любовь', 'дружба']},\n",
        "    {'text': 'Данные 2', 'values': ['семья', 'деньги']},\n",
        "]\n",
        "\n",
        "# Создание словаря ценностей\n",
        "values = [\"любовь\", \"семья\", \"дружба\", \"деньги\", ]\n",
        "\n",
        "# Предобработка текста и создание меток\n",
        "texts = [data['text'] for data in dataset]\n",
        "labels = []\n",
        "for data in dataset:\n",
        "    label = [0] * len(values)\n",
        "    for value in data['values']:\n",
        "        if value in values:\n",
        "            idx = values.index(value)\n",
        "            label[idx] = 1\n",
        "    labels.append(label)\n",
        "\n",
        "# Разделение на обучающий и тестовый наборы\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Предобработка текста и создание словаря слов\n",
        "def preprocess_text(text):\n",
        "    # Ваша логика предобработки текста\n",
        "    return text.lower()\n",
        "\n",
        "def create_word_vocab(texts):\n",
        "    word_to_idx = {}\n",
        "    for text in texts:\n",
        "        processed_text = preprocess_text(text)\n",
        "        for word in processed_text:\n",
        "            if word not in word_to_idx:\n",
        "                word_to_idx[word] = len(word_to_idx)\n",
        "    return word_to_idx\n",
        "\n",
        "# Создание словаря слов\n",
        "word_to_idx = create_word_vocab(train_texts)\n",
        "\n",
        "# Преобразование текста в векторы признаков\n",
        "def text_to_vector(text, word_to_idx):\n",
        "    processed_text = preprocess_text(text)\n",
        "    vector = [0] * len(word_to_idx)\n",
        "    for word in processed_text:\n",
        "        if word in word_to_idx:\n",
        "            idx = word_to_idx[word]\n",
        "            vector[idx] = 1\n",
        "    return vector\n",
        "\n",
        "# Преобразование текста обучающего и тестового наборов в векторы признаков\n",
        "train_vectors = [text_to_vector(text, word_to_idx) for text in train_texts]\n",
        "test_vectors = [text_to_vector(text, word_to_idx) for text in test_texts]\n",
        "\n",
        "# Конвертация в тензоры PyTorch\n",
        "train_data = torch.tensor(train_vectors, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "test_data = torch.tensor(test_vectors, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
        "\n",
        "# Определение модели NLP\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Обучение модели\n",
        "input_dim = len(word_to_idx)\n",
        "hidden_dim = 128\n",
        "output_dim = len(values)\n",
        "\n",
        "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 1000\n",
        "batch_size = 32\n",
        "\n",
        "num_batches = len(train_data) / batch_size\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch_idx in range(int(num_batches)):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "\n",
        "        batch_data = train_data[start_idx:end_idx]\n",
        "        batch_labels = train_labels[start_idx:end_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/num_batches}\")\n",
        "\n",
        "\n",
        "# Функция для предсказания\n",
        "def predict(text, model, word_to_idx, values):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vector = text_to_vector(text, word_to_idx)\n",
        "        data = torch.tensor(vector, dtype=torch.float32).unsqueeze(0)\n",
        "        outputs = model(data)\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted_labels = (probabilities > 0.5).squeeze().tolist()\n",
        "        predicted_values = [values[i] for i, label in enumerate(predicted_labels) if label]\n",
        "        return predicted_values\n",
        "\n",
        "# Пример использования\n",
        "example_text = \"Сегодня в новостях рассказывают о любви и семье. Также упоминается важность дружбы и денег.\"\n",
        "predicted_values = predict(example_text, model, word_to_idx, values)\n",
        "print(f\"Текст: {example_text}\")\n",
        "print(\"Классификация:\", predicted_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKI7J2eCxp1T",
        "outputId": "87a66f8d-8f73-481b-a624-2d460c8b1bdd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Loss: 0.0\n",
            "Epoch 2/1000, Loss: 0.0\n",
            "Epoch 3/1000, Loss: 0.0\n",
            "Epoch 4/1000, Loss: 0.0\n",
            "Epoch 5/1000, Loss: 0.0\n",
            "Epoch 6/1000, Loss: 0.0\n",
            "Epoch 7/1000, Loss: 0.0\n",
            "Epoch 8/1000, Loss: 0.0\n",
            "Epoch 9/1000, Loss: 0.0\n",
            "Epoch 10/1000, Loss: 0.0\n",
            "Epoch 11/1000, Loss: 0.0\n",
            "Epoch 12/1000, Loss: 0.0\n",
            "Epoch 13/1000, Loss: 0.0\n",
            "Epoch 14/1000, Loss: 0.0\n",
            "Epoch 15/1000, Loss: 0.0\n",
            "Epoch 16/1000, Loss: 0.0\n",
            "Epoch 17/1000, Loss: 0.0\n",
            "Epoch 18/1000, Loss: 0.0\n",
            "Epoch 19/1000, Loss: 0.0\n",
            "Epoch 20/1000, Loss: 0.0\n",
            "Epoch 21/1000, Loss: 0.0\n",
            "Epoch 22/1000, Loss: 0.0\n",
            "Epoch 23/1000, Loss: 0.0\n",
            "Epoch 24/1000, Loss: 0.0\n",
            "Epoch 25/1000, Loss: 0.0\n",
            "Epoch 26/1000, Loss: 0.0\n",
            "Epoch 27/1000, Loss: 0.0\n",
            "Epoch 28/1000, Loss: 0.0\n",
            "Epoch 29/1000, Loss: 0.0\n",
            "Epoch 30/1000, Loss: 0.0\n",
            "Epoch 31/1000, Loss: 0.0\n",
            "Epoch 32/1000, Loss: 0.0\n",
            "Epoch 33/1000, Loss: 0.0\n",
            "Epoch 34/1000, Loss: 0.0\n",
            "Epoch 35/1000, Loss: 0.0\n",
            "Epoch 36/1000, Loss: 0.0\n",
            "Epoch 37/1000, Loss: 0.0\n",
            "Epoch 38/1000, Loss: 0.0\n",
            "Epoch 39/1000, Loss: 0.0\n",
            "Epoch 40/1000, Loss: 0.0\n",
            "Epoch 41/1000, Loss: 0.0\n",
            "Epoch 42/1000, Loss: 0.0\n",
            "Epoch 43/1000, Loss: 0.0\n",
            "Epoch 44/1000, Loss: 0.0\n",
            "Epoch 45/1000, Loss: 0.0\n",
            "Epoch 46/1000, Loss: 0.0\n",
            "Epoch 47/1000, Loss: 0.0\n",
            "Epoch 48/1000, Loss: 0.0\n",
            "Epoch 49/1000, Loss: 0.0\n",
            "Epoch 50/1000, Loss: 0.0\n",
            "Epoch 51/1000, Loss: 0.0\n",
            "Epoch 52/1000, Loss: 0.0\n",
            "Epoch 53/1000, Loss: 0.0\n",
            "Epoch 54/1000, Loss: 0.0\n",
            "Epoch 55/1000, Loss: 0.0\n",
            "Epoch 56/1000, Loss: 0.0\n",
            "Epoch 57/1000, Loss: 0.0\n",
            "Epoch 58/1000, Loss: 0.0\n",
            "Epoch 59/1000, Loss: 0.0\n",
            "Epoch 60/1000, Loss: 0.0\n",
            "Epoch 61/1000, Loss: 0.0\n",
            "Epoch 62/1000, Loss: 0.0\n",
            "Epoch 63/1000, Loss: 0.0\n",
            "Epoch 64/1000, Loss: 0.0\n",
            "Epoch 65/1000, Loss: 0.0\n",
            "Epoch 66/1000, Loss: 0.0\n",
            "Epoch 67/1000, Loss: 0.0\n",
            "Epoch 68/1000, Loss: 0.0\n",
            "Epoch 69/1000, Loss: 0.0\n",
            "Epoch 70/1000, Loss: 0.0\n",
            "Epoch 71/1000, Loss: 0.0\n",
            "Epoch 72/1000, Loss: 0.0\n",
            "Epoch 73/1000, Loss: 0.0\n",
            "Epoch 74/1000, Loss: 0.0\n",
            "Epoch 75/1000, Loss: 0.0\n",
            "Epoch 76/1000, Loss: 0.0\n",
            "Epoch 77/1000, Loss: 0.0\n",
            "Epoch 78/1000, Loss: 0.0\n",
            "Epoch 79/1000, Loss: 0.0\n",
            "Epoch 80/1000, Loss: 0.0\n",
            "Epoch 81/1000, Loss: 0.0\n",
            "Epoch 82/1000, Loss: 0.0\n",
            "Epoch 83/1000, Loss: 0.0\n",
            "Epoch 84/1000, Loss: 0.0\n",
            "Epoch 85/1000, Loss: 0.0\n",
            "Epoch 86/1000, Loss: 0.0\n",
            "Epoch 87/1000, Loss: 0.0\n",
            "Epoch 88/1000, Loss: 0.0\n",
            "Epoch 89/1000, Loss: 0.0\n",
            "Epoch 90/1000, Loss: 0.0\n",
            "Epoch 91/1000, Loss: 0.0\n",
            "Epoch 92/1000, Loss: 0.0\n",
            "Epoch 93/1000, Loss: 0.0\n",
            "Epoch 94/1000, Loss: 0.0\n",
            "Epoch 95/1000, Loss: 0.0\n",
            "Epoch 96/1000, Loss: 0.0\n",
            "Epoch 97/1000, Loss: 0.0\n",
            "Epoch 98/1000, Loss: 0.0\n",
            "Epoch 99/1000, Loss: 0.0\n",
            "Epoch 100/1000, Loss: 0.0\n",
            "Epoch 101/1000, Loss: 0.0\n",
            "Epoch 102/1000, Loss: 0.0\n",
            "Epoch 103/1000, Loss: 0.0\n",
            "Epoch 104/1000, Loss: 0.0\n",
            "Epoch 105/1000, Loss: 0.0\n",
            "Epoch 106/1000, Loss: 0.0\n",
            "Epoch 107/1000, Loss: 0.0\n",
            "Epoch 108/1000, Loss: 0.0\n",
            "Epoch 109/1000, Loss: 0.0\n",
            "Epoch 110/1000, Loss: 0.0\n",
            "Epoch 111/1000, Loss: 0.0\n",
            "Epoch 112/1000, Loss: 0.0\n",
            "Epoch 113/1000, Loss: 0.0\n",
            "Epoch 114/1000, Loss: 0.0\n",
            "Epoch 115/1000, Loss: 0.0\n",
            "Epoch 116/1000, Loss: 0.0\n",
            "Epoch 117/1000, Loss: 0.0\n",
            "Epoch 118/1000, Loss: 0.0\n",
            "Epoch 119/1000, Loss: 0.0\n",
            "Epoch 120/1000, Loss: 0.0\n",
            "Epoch 121/1000, Loss: 0.0\n",
            "Epoch 122/1000, Loss: 0.0\n",
            "Epoch 123/1000, Loss: 0.0\n",
            "Epoch 124/1000, Loss: 0.0\n",
            "Epoch 125/1000, Loss: 0.0\n",
            "Epoch 126/1000, Loss: 0.0\n",
            "Epoch 127/1000, Loss: 0.0\n",
            "Epoch 128/1000, Loss: 0.0\n",
            "Epoch 129/1000, Loss: 0.0\n",
            "Epoch 130/1000, Loss: 0.0\n",
            "Epoch 131/1000, Loss: 0.0\n",
            "Epoch 132/1000, Loss: 0.0\n",
            "Epoch 133/1000, Loss: 0.0\n",
            "Epoch 134/1000, Loss: 0.0\n",
            "Epoch 135/1000, Loss: 0.0\n",
            "Epoch 136/1000, Loss: 0.0\n",
            "Epoch 137/1000, Loss: 0.0\n",
            "Epoch 138/1000, Loss: 0.0\n",
            "Epoch 139/1000, Loss: 0.0\n",
            "Epoch 140/1000, Loss: 0.0\n",
            "Epoch 141/1000, Loss: 0.0\n",
            "Epoch 142/1000, Loss: 0.0\n",
            "Epoch 143/1000, Loss: 0.0\n",
            "Epoch 144/1000, Loss: 0.0\n",
            "Epoch 145/1000, Loss: 0.0\n",
            "Epoch 146/1000, Loss: 0.0\n",
            "Epoch 147/1000, Loss: 0.0\n",
            "Epoch 148/1000, Loss: 0.0\n",
            "Epoch 149/1000, Loss: 0.0\n",
            "Epoch 150/1000, Loss: 0.0\n",
            "Epoch 151/1000, Loss: 0.0\n",
            "Epoch 152/1000, Loss: 0.0\n",
            "Epoch 153/1000, Loss: 0.0\n",
            "Epoch 154/1000, Loss: 0.0\n",
            "Epoch 155/1000, Loss: 0.0\n",
            "Epoch 156/1000, Loss: 0.0\n",
            "Epoch 157/1000, Loss: 0.0\n",
            "Epoch 158/1000, Loss: 0.0\n",
            "Epoch 159/1000, Loss: 0.0\n",
            "Epoch 160/1000, Loss: 0.0\n",
            "Epoch 161/1000, Loss: 0.0\n",
            "Epoch 162/1000, Loss: 0.0\n",
            "Epoch 163/1000, Loss: 0.0\n",
            "Epoch 164/1000, Loss: 0.0\n",
            "Epoch 165/1000, Loss: 0.0\n",
            "Epoch 166/1000, Loss: 0.0\n",
            "Epoch 167/1000, Loss: 0.0\n",
            "Epoch 168/1000, Loss: 0.0\n",
            "Epoch 169/1000, Loss: 0.0\n",
            "Epoch 170/1000, Loss: 0.0\n",
            "Epoch 171/1000, Loss: 0.0\n",
            "Epoch 172/1000, Loss: 0.0\n",
            "Epoch 173/1000, Loss: 0.0\n",
            "Epoch 174/1000, Loss: 0.0\n",
            "Epoch 175/1000, Loss: 0.0\n",
            "Epoch 176/1000, Loss: 0.0\n",
            "Epoch 177/1000, Loss: 0.0\n",
            "Epoch 178/1000, Loss: 0.0\n",
            "Epoch 179/1000, Loss: 0.0\n",
            "Epoch 180/1000, Loss: 0.0\n",
            "Epoch 181/1000, Loss: 0.0\n",
            "Epoch 182/1000, Loss: 0.0\n",
            "Epoch 183/1000, Loss: 0.0\n",
            "Epoch 184/1000, Loss: 0.0\n",
            "Epoch 185/1000, Loss: 0.0\n",
            "Epoch 186/1000, Loss: 0.0\n",
            "Epoch 187/1000, Loss: 0.0\n",
            "Epoch 188/1000, Loss: 0.0\n",
            "Epoch 189/1000, Loss: 0.0\n",
            "Epoch 190/1000, Loss: 0.0\n",
            "Epoch 191/1000, Loss: 0.0\n",
            "Epoch 192/1000, Loss: 0.0\n",
            "Epoch 193/1000, Loss: 0.0\n",
            "Epoch 194/1000, Loss: 0.0\n",
            "Epoch 195/1000, Loss: 0.0\n",
            "Epoch 196/1000, Loss: 0.0\n",
            "Epoch 197/1000, Loss: 0.0\n",
            "Epoch 198/1000, Loss: 0.0\n",
            "Epoch 199/1000, Loss: 0.0\n",
            "Epoch 200/1000, Loss: 0.0\n",
            "Epoch 201/1000, Loss: 0.0\n",
            "Epoch 202/1000, Loss: 0.0\n",
            "Epoch 203/1000, Loss: 0.0\n",
            "Epoch 204/1000, Loss: 0.0\n",
            "Epoch 205/1000, Loss: 0.0\n",
            "Epoch 206/1000, Loss: 0.0\n",
            "Epoch 207/1000, Loss: 0.0\n",
            "Epoch 208/1000, Loss: 0.0\n",
            "Epoch 209/1000, Loss: 0.0\n",
            "Epoch 210/1000, Loss: 0.0\n",
            "Epoch 211/1000, Loss: 0.0\n",
            "Epoch 212/1000, Loss: 0.0\n",
            "Epoch 213/1000, Loss: 0.0\n",
            "Epoch 214/1000, Loss: 0.0\n",
            "Epoch 215/1000, Loss: 0.0\n",
            "Epoch 216/1000, Loss: 0.0\n",
            "Epoch 217/1000, Loss: 0.0\n",
            "Epoch 218/1000, Loss: 0.0\n",
            "Epoch 219/1000, Loss: 0.0\n",
            "Epoch 220/1000, Loss: 0.0\n",
            "Epoch 221/1000, Loss: 0.0\n",
            "Epoch 222/1000, Loss: 0.0\n",
            "Epoch 223/1000, Loss: 0.0\n",
            "Epoch 224/1000, Loss: 0.0\n",
            "Epoch 225/1000, Loss: 0.0\n",
            "Epoch 226/1000, Loss: 0.0\n",
            "Epoch 227/1000, Loss: 0.0\n",
            "Epoch 228/1000, Loss: 0.0\n",
            "Epoch 229/1000, Loss: 0.0\n",
            "Epoch 230/1000, Loss: 0.0\n",
            "Epoch 231/1000, Loss: 0.0\n",
            "Epoch 232/1000, Loss: 0.0\n",
            "Epoch 233/1000, Loss: 0.0\n",
            "Epoch 234/1000, Loss: 0.0\n",
            "Epoch 235/1000, Loss: 0.0\n",
            "Epoch 236/1000, Loss: 0.0\n",
            "Epoch 237/1000, Loss: 0.0\n",
            "Epoch 238/1000, Loss: 0.0\n",
            "Epoch 239/1000, Loss: 0.0\n",
            "Epoch 240/1000, Loss: 0.0\n",
            "Epoch 241/1000, Loss: 0.0\n",
            "Epoch 242/1000, Loss: 0.0\n",
            "Epoch 243/1000, Loss: 0.0\n",
            "Epoch 244/1000, Loss: 0.0\n",
            "Epoch 245/1000, Loss: 0.0\n",
            "Epoch 246/1000, Loss: 0.0\n",
            "Epoch 247/1000, Loss: 0.0\n",
            "Epoch 248/1000, Loss: 0.0\n",
            "Epoch 249/1000, Loss: 0.0\n",
            "Epoch 250/1000, Loss: 0.0\n",
            "Epoch 251/1000, Loss: 0.0\n",
            "Epoch 252/1000, Loss: 0.0\n",
            "Epoch 253/1000, Loss: 0.0\n",
            "Epoch 254/1000, Loss: 0.0\n",
            "Epoch 255/1000, Loss: 0.0\n",
            "Epoch 256/1000, Loss: 0.0\n",
            "Epoch 257/1000, Loss: 0.0\n",
            "Epoch 258/1000, Loss: 0.0\n",
            "Epoch 259/1000, Loss: 0.0\n",
            "Epoch 260/1000, Loss: 0.0\n",
            "Epoch 261/1000, Loss: 0.0\n",
            "Epoch 262/1000, Loss: 0.0\n",
            "Epoch 263/1000, Loss: 0.0\n",
            "Epoch 264/1000, Loss: 0.0\n",
            "Epoch 265/1000, Loss: 0.0\n",
            "Epoch 266/1000, Loss: 0.0\n",
            "Epoch 267/1000, Loss: 0.0\n",
            "Epoch 268/1000, Loss: 0.0\n",
            "Epoch 269/1000, Loss: 0.0\n",
            "Epoch 270/1000, Loss: 0.0\n",
            "Epoch 271/1000, Loss: 0.0\n",
            "Epoch 272/1000, Loss: 0.0\n",
            "Epoch 273/1000, Loss: 0.0\n",
            "Epoch 274/1000, Loss: 0.0\n",
            "Epoch 275/1000, Loss: 0.0\n",
            "Epoch 276/1000, Loss: 0.0\n",
            "Epoch 277/1000, Loss: 0.0\n",
            "Epoch 278/1000, Loss: 0.0\n",
            "Epoch 279/1000, Loss: 0.0\n",
            "Epoch 280/1000, Loss: 0.0\n",
            "Epoch 281/1000, Loss: 0.0\n",
            "Epoch 282/1000, Loss: 0.0\n",
            "Epoch 283/1000, Loss: 0.0\n",
            "Epoch 284/1000, Loss: 0.0\n",
            "Epoch 285/1000, Loss: 0.0\n",
            "Epoch 286/1000, Loss: 0.0\n",
            "Epoch 287/1000, Loss: 0.0\n",
            "Epoch 288/1000, Loss: 0.0\n",
            "Epoch 289/1000, Loss: 0.0\n",
            "Epoch 290/1000, Loss: 0.0\n",
            "Epoch 291/1000, Loss: 0.0\n",
            "Epoch 292/1000, Loss: 0.0\n",
            "Epoch 293/1000, Loss: 0.0\n",
            "Epoch 294/1000, Loss: 0.0\n",
            "Epoch 295/1000, Loss: 0.0\n",
            "Epoch 296/1000, Loss: 0.0\n",
            "Epoch 297/1000, Loss: 0.0\n",
            "Epoch 298/1000, Loss: 0.0\n",
            "Epoch 299/1000, Loss: 0.0\n",
            "Epoch 300/1000, Loss: 0.0\n",
            "Epoch 301/1000, Loss: 0.0\n",
            "Epoch 302/1000, Loss: 0.0\n",
            "Epoch 303/1000, Loss: 0.0\n",
            "Epoch 304/1000, Loss: 0.0\n",
            "Epoch 305/1000, Loss: 0.0\n",
            "Epoch 306/1000, Loss: 0.0\n",
            "Epoch 307/1000, Loss: 0.0\n",
            "Epoch 308/1000, Loss: 0.0\n",
            "Epoch 309/1000, Loss: 0.0\n",
            "Epoch 310/1000, Loss: 0.0\n",
            "Epoch 311/1000, Loss: 0.0\n",
            "Epoch 312/1000, Loss: 0.0\n",
            "Epoch 313/1000, Loss: 0.0\n",
            "Epoch 314/1000, Loss: 0.0\n",
            "Epoch 315/1000, Loss: 0.0\n",
            "Epoch 316/1000, Loss: 0.0\n",
            "Epoch 317/1000, Loss: 0.0\n",
            "Epoch 318/1000, Loss: 0.0\n",
            "Epoch 319/1000, Loss: 0.0\n",
            "Epoch 320/1000, Loss: 0.0\n",
            "Epoch 321/1000, Loss: 0.0\n",
            "Epoch 322/1000, Loss: 0.0\n",
            "Epoch 323/1000, Loss: 0.0\n",
            "Epoch 324/1000, Loss: 0.0\n",
            "Epoch 325/1000, Loss: 0.0\n",
            "Epoch 326/1000, Loss: 0.0\n",
            "Epoch 327/1000, Loss: 0.0\n",
            "Epoch 328/1000, Loss: 0.0\n",
            "Epoch 329/1000, Loss: 0.0\n",
            "Epoch 330/1000, Loss: 0.0\n",
            "Epoch 331/1000, Loss: 0.0\n",
            "Epoch 332/1000, Loss: 0.0\n",
            "Epoch 333/1000, Loss: 0.0\n",
            "Epoch 334/1000, Loss: 0.0\n",
            "Epoch 335/1000, Loss: 0.0\n",
            "Epoch 336/1000, Loss: 0.0\n",
            "Epoch 337/1000, Loss: 0.0\n",
            "Epoch 338/1000, Loss: 0.0\n",
            "Epoch 339/1000, Loss: 0.0\n",
            "Epoch 340/1000, Loss: 0.0\n",
            "Epoch 341/1000, Loss: 0.0\n",
            "Epoch 342/1000, Loss: 0.0\n",
            "Epoch 343/1000, Loss: 0.0\n",
            "Epoch 344/1000, Loss: 0.0\n",
            "Epoch 345/1000, Loss: 0.0\n",
            "Epoch 346/1000, Loss: 0.0\n",
            "Epoch 347/1000, Loss: 0.0\n",
            "Epoch 348/1000, Loss: 0.0\n",
            "Epoch 349/1000, Loss: 0.0\n",
            "Epoch 350/1000, Loss: 0.0\n",
            "Epoch 351/1000, Loss: 0.0\n",
            "Epoch 352/1000, Loss: 0.0\n",
            "Epoch 353/1000, Loss: 0.0\n",
            "Epoch 354/1000, Loss: 0.0\n",
            "Epoch 355/1000, Loss: 0.0\n",
            "Epoch 356/1000, Loss: 0.0\n",
            "Epoch 357/1000, Loss: 0.0\n",
            "Epoch 358/1000, Loss: 0.0\n",
            "Epoch 359/1000, Loss: 0.0\n",
            "Epoch 360/1000, Loss: 0.0\n",
            "Epoch 361/1000, Loss: 0.0\n",
            "Epoch 362/1000, Loss: 0.0\n",
            "Epoch 363/1000, Loss: 0.0\n",
            "Epoch 364/1000, Loss: 0.0\n",
            "Epoch 365/1000, Loss: 0.0\n",
            "Epoch 366/1000, Loss: 0.0\n",
            "Epoch 367/1000, Loss: 0.0\n",
            "Epoch 368/1000, Loss: 0.0\n",
            "Epoch 369/1000, Loss: 0.0\n",
            "Epoch 370/1000, Loss: 0.0\n",
            "Epoch 371/1000, Loss: 0.0\n",
            "Epoch 372/1000, Loss: 0.0\n",
            "Epoch 373/1000, Loss: 0.0\n",
            "Epoch 374/1000, Loss: 0.0\n",
            "Epoch 375/1000, Loss: 0.0\n",
            "Epoch 376/1000, Loss: 0.0\n",
            "Epoch 377/1000, Loss: 0.0\n",
            "Epoch 378/1000, Loss: 0.0\n",
            "Epoch 379/1000, Loss: 0.0\n",
            "Epoch 380/1000, Loss: 0.0\n",
            "Epoch 381/1000, Loss: 0.0\n",
            "Epoch 382/1000, Loss: 0.0\n",
            "Epoch 383/1000, Loss: 0.0\n",
            "Epoch 384/1000, Loss: 0.0\n",
            "Epoch 385/1000, Loss: 0.0\n",
            "Epoch 386/1000, Loss: 0.0\n",
            "Epoch 387/1000, Loss: 0.0\n",
            "Epoch 388/1000, Loss: 0.0\n",
            "Epoch 389/1000, Loss: 0.0\n",
            "Epoch 390/1000, Loss: 0.0\n",
            "Epoch 391/1000, Loss: 0.0\n",
            "Epoch 392/1000, Loss: 0.0\n",
            "Epoch 393/1000, Loss: 0.0\n",
            "Epoch 394/1000, Loss: 0.0\n",
            "Epoch 395/1000, Loss: 0.0\n",
            "Epoch 396/1000, Loss: 0.0\n",
            "Epoch 397/1000, Loss: 0.0\n",
            "Epoch 398/1000, Loss: 0.0\n",
            "Epoch 399/1000, Loss: 0.0\n",
            "Epoch 400/1000, Loss: 0.0\n",
            "Epoch 401/1000, Loss: 0.0\n",
            "Epoch 402/1000, Loss: 0.0\n",
            "Epoch 403/1000, Loss: 0.0\n",
            "Epoch 404/1000, Loss: 0.0\n",
            "Epoch 405/1000, Loss: 0.0\n",
            "Epoch 406/1000, Loss: 0.0\n",
            "Epoch 407/1000, Loss: 0.0\n",
            "Epoch 408/1000, Loss: 0.0\n",
            "Epoch 409/1000, Loss: 0.0\n",
            "Epoch 410/1000, Loss: 0.0\n",
            "Epoch 411/1000, Loss: 0.0\n",
            "Epoch 412/1000, Loss: 0.0\n",
            "Epoch 413/1000, Loss: 0.0\n",
            "Epoch 414/1000, Loss: 0.0\n",
            "Epoch 415/1000, Loss: 0.0\n",
            "Epoch 416/1000, Loss: 0.0\n",
            "Epoch 417/1000, Loss: 0.0\n",
            "Epoch 418/1000, Loss: 0.0\n",
            "Epoch 419/1000, Loss: 0.0\n",
            "Epoch 420/1000, Loss: 0.0\n",
            "Epoch 421/1000, Loss: 0.0\n",
            "Epoch 422/1000, Loss: 0.0\n",
            "Epoch 423/1000, Loss: 0.0\n",
            "Epoch 424/1000, Loss: 0.0\n",
            "Epoch 425/1000, Loss: 0.0\n",
            "Epoch 426/1000, Loss: 0.0\n",
            "Epoch 427/1000, Loss: 0.0\n",
            "Epoch 428/1000, Loss: 0.0\n",
            "Epoch 429/1000, Loss: 0.0\n",
            "Epoch 430/1000, Loss: 0.0\n",
            "Epoch 431/1000, Loss: 0.0\n",
            "Epoch 432/1000, Loss: 0.0\n",
            "Epoch 433/1000, Loss: 0.0\n",
            "Epoch 434/1000, Loss: 0.0\n",
            "Epoch 435/1000, Loss: 0.0\n",
            "Epoch 436/1000, Loss: 0.0\n",
            "Epoch 437/1000, Loss: 0.0\n",
            "Epoch 438/1000, Loss: 0.0\n",
            "Epoch 439/1000, Loss: 0.0\n",
            "Epoch 440/1000, Loss: 0.0\n",
            "Epoch 441/1000, Loss: 0.0\n",
            "Epoch 442/1000, Loss: 0.0\n",
            "Epoch 443/1000, Loss: 0.0\n",
            "Epoch 444/1000, Loss: 0.0\n",
            "Epoch 445/1000, Loss: 0.0\n",
            "Epoch 446/1000, Loss: 0.0\n",
            "Epoch 447/1000, Loss: 0.0\n",
            "Epoch 448/1000, Loss: 0.0\n",
            "Epoch 449/1000, Loss: 0.0\n",
            "Epoch 450/1000, Loss: 0.0\n",
            "Epoch 451/1000, Loss: 0.0\n",
            "Epoch 452/1000, Loss: 0.0\n",
            "Epoch 453/1000, Loss: 0.0\n",
            "Epoch 454/1000, Loss: 0.0\n",
            "Epoch 455/1000, Loss: 0.0\n",
            "Epoch 456/1000, Loss: 0.0\n",
            "Epoch 457/1000, Loss: 0.0\n",
            "Epoch 458/1000, Loss: 0.0\n",
            "Epoch 459/1000, Loss: 0.0\n",
            "Epoch 460/1000, Loss: 0.0\n",
            "Epoch 461/1000, Loss: 0.0\n",
            "Epoch 462/1000, Loss: 0.0\n",
            "Epoch 463/1000, Loss: 0.0\n",
            "Epoch 464/1000, Loss: 0.0\n",
            "Epoch 465/1000, Loss: 0.0\n",
            "Epoch 466/1000, Loss: 0.0\n",
            "Epoch 467/1000, Loss: 0.0\n",
            "Epoch 468/1000, Loss: 0.0\n",
            "Epoch 469/1000, Loss: 0.0\n",
            "Epoch 470/1000, Loss: 0.0\n",
            "Epoch 471/1000, Loss: 0.0\n",
            "Epoch 472/1000, Loss: 0.0\n",
            "Epoch 473/1000, Loss: 0.0\n",
            "Epoch 474/1000, Loss: 0.0\n",
            "Epoch 475/1000, Loss: 0.0\n",
            "Epoch 476/1000, Loss: 0.0\n",
            "Epoch 477/1000, Loss: 0.0\n",
            "Epoch 478/1000, Loss: 0.0\n",
            "Epoch 479/1000, Loss: 0.0\n",
            "Epoch 480/1000, Loss: 0.0\n",
            "Epoch 481/1000, Loss: 0.0\n",
            "Epoch 482/1000, Loss: 0.0\n",
            "Epoch 483/1000, Loss: 0.0\n",
            "Epoch 484/1000, Loss: 0.0\n",
            "Epoch 485/1000, Loss: 0.0\n",
            "Epoch 486/1000, Loss: 0.0\n",
            "Epoch 487/1000, Loss: 0.0\n",
            "Epoch 488/1000, Loss: 0.0\n",
            "Epoch 489/1000, Loss: 0.0\n",
            "Epoch 490/1000, Loss: 0.0\n",
            "Epoch 491/1000, Loss: 0.0\n",
            "Epoch 492/1000, Loss: 0.0\n",
            "Epoch 493/1000, Loss: 0.0\n",
            "Epoch 494/1000, Loss: 0.0\n",
            "Epoch 495/1000, Loss: 0.0\n",
            "Epoch 496/1000, Loss: 0.0\n",
            "Epoch 497/1000, Loss: 0.0\n",
            "Epoch 498/1000, Loss: 0.0\n",
            "Epoch 499/1000, Loss: 0.0\n",
            "Epoch 500/1000, Loss: 0.0\n",
            "Epoch 501/1000, Loss: 0.0\n",
            "Epoch 502/1000, Loss: 0.0\n",
            "Epoch 503/1000, Loss: 0.0\n",
            "Epoch 504/1000, Loss: 0.0\n",
            "Epoch 505/1000, Loss: 0.0\n",
            "Epoch 506/1000, Loss: 0.0\n",
            "Epoch 507/1000, Loss: 0.0\n",
            "Epoch 508/1000, Loss: 0.0\n",
            "Epoch 509/1000, Loss: 0.0\n",
            "Epoch 510/1000, Loss: 0.0\n",
            "Epoch 511/1000, Loss: 0.0\n",
            "Epoch 512/1000, Loss: 0.0\n",
            "Epoch 513/1000, Loss: 0.0\n",
            "Epoch 514/1000, Loss: 0.0\n",
            "Epoch 515/1000, Loss: 0.0\n",
            "Epoch 516/1000, Loss: 0.0\n",
            "Epoch 517/1000, Loss: 0.0\n",
            "Epoch 518/1000, Loss: 0.0\n",
            "Epoch 519/1000, Loss: 0.0\n",
            "Epoch 520/1000, Loss: 0.0\n",
            "Epoch 521/1000, Loss: 0.0\n",
            "Epoch 522/1000, Loss: 0.0\n",
            "Epoch 523/1000, Loss: 0.0\n",
            "Epoch 524/1000, Loss: 0.0\n",
            "Epoch 525/1000, Loss: 0.0\n",
            "Epoch 526/1000, Loss: 0.0\n",
            "Epoch 527/1000, Loss: 0.0\n",
            "Epoch 528/1000, Loss: 0.0\n",
            "Epoch 529/1000, Loss: 0.0\n",
            "Epoch 530/1000, Loss: 0.0\n",
            "Epoch 531/1000, Loss: 0.0\n",
            "Epoch 532/1000, Loss: 0.0\n",
            "Epoch 533/1000, Loss: 0.0\n",
            "Epoch 534/1000, Loss: 0.0\n",
            "Epoch 535/1000, Loss: 0.0\n",
            "Epoch 536/1000, Loss: 0.0\n",
            "Epoch 537/1000, Loss: 0.0\n",
            "Epoch 538/1000, Loss: 0.0\n",
            "Epoch 539/1000, Loss: 0.0\n",
            "Epoch 540/1000, Loss: 0.0\n",
            "Epoch 541/1000, Loss: 0.0\n",
            "Epoch 542/1000, Loss: 0.0\n",
            "Epoch 543/1000, Loss: 0.0\n",
            "Epoch 544/1000, Loss: 0.0\n",
            "Epoch 545/1000, Loss: 0.0\n",
            "Epoch 546/1000, Loss: 0.0\n",
            "Epoch 547/1000, Loss: 0.0\n",
            "Epoch 548/1000, Loss: 0.0\n",
            "Epoch 549/1000, Loss: 0.0\n",
            "Epoch 550/1000, Loss: 0.0\n",
            "Epoch 551/1000, Loss: 0.0\n",
            "Epoch 552/1000, Loss: 0.0\n",
            "Epoch 553/1000, Loss: 0.0\n",
            "Epoch 554/1000, Loss: 0.0\n",
            "Epoch 555/1000, Loss: 0.0\n",
            "Epoch 556/1000, Loss: 0.0\n",
            "Epoch 557/1000, Loss: 0.0\n",
            "Epoch 558/1000, Loss: 0.0\n",
            "Epoch 559/1000, Loss: 0.0\n",
            "Epoch 560/1000, Loss: 0.0\n",
            "Epoch 561/1000, Loss: 0.0\n",
            "Epoch 562/1000, Loss: 0.0\n",
            "Epoch 563/1000, Loss: 0.0\n",
            "Epoch 564/1000, Loss: 0.0\n",
            "Epoch 565/1000, Loss: 0.0\n",
            "Epoch 566/1000, Loss: 0.0\n",
            "Epoch 567/1000, Loss: 0.0\n",
            "Epoch 568/1000, Loss: 0.0\n",
            "Epoch 569/1000, Loss: 0.0\n",
            "Epoch 570/1000, Loss: 0.0\n",
            "Epoch 571/1000, Loss: 0.0\n",
            "Epoch 572/1000, Loss: 0.0\n",
            "Epoch 573/1000, Loss: 0.0\n",
            "Epoch 574/1000, Loss: 0.0\n",
            "Epoch 575/1000, Loss: 0.0\n",
            "Epoch 576/1000, Loss: 0.0\n",
            "Epoch 577/1000, Loss: 0.0\n",
            "Epoch 578/1000, Loss: 0.0\n",
            "Epoch 579/1000, Loss: 0.0\n",
            "Epoch 580/1000, Loss: 0.0\n",
            "Epoch 581/1000, Loss: 0.0\n",
            "Epoch 582/1000, Loss: 0.0\n",
            "Epoch 583/1000, Loss: 0.0\n",
            "Epoch 584/1000, Loss: 0.0\n",
            "Epoch 585/1000, Loss: 0.0\n",
            "Epoch 586/1000, Loss: 0.0\n",
            "Epoch 587/1000, Loss: 0.0\n",
            "Epoch 588/1000, Loss: 0.0\n",
            "Epoch 589/1000, Loss: 0.0\n",
            "Epoch 590/1000, Loss: 0.0\n",
            "Epoch 591/1000, Loss: 0.0\n",
            "Epoch 592/1000, Loss: 0.0\n",
            "Epoch 593/1000, Loss: 0.0\n",
            "Epoch 594/1000, Loss: 0.0\n",
            "Epoch 595/1000, Loss: 0.0\n",
            "Epoch 596/1000, Loss: 0.0\n",
            "Epoch 597/1000, Loss: 0.0\n",
            "Epoch 598/1000, Loss: 0.0\n",
            "Epoch 599/1000, Loss: 0.0\n",
            "Epoch 600/1000, Loss: 0.0\n",
            "Epoch 601/1000, Loss: 0.0\n",
            "Epoch 602/1000, Loss: 0.0\n",
            "Epoch 603/1000, Loss: 0.0\n",
            "Epoch 604/1000, Loss: 0.0\n",
            "Epoch 605/1000, Loss: 0.0\n",
            "Epoch 606/1000, Loss: 0.0\n",
            "Epoch 607/1000, Loss: 0.0\n",
            "Epoch 608/1000, Loss: 0.0\n",
            "Epoch 609/1000, Loss: 0.0\n",
            "Epoch 610/1000, Loss: 0.0\n",
            "Epoch 611/1000, Loss: 0.0\n",
            "Epoch 612/1000, Loss: 0.0\n",
            "Epoch 613/1000, Loss: 0.0\n",
            "Epoch 614/1000, Loss: 0.0\n",
            "Epoch 615/1000, Loss: 0.0\n",
            "Epoch 616/1000, Loss: 0.0\n",
            "Epoch 617/1000, Loss: 0.0\n",
            "Epoch 618/1000, Loss: 0.0\n",
            "Epoch 619/1000, Loss: 0.0\n",
            "Epoch 620/1000, Loss: 0.0\n",
            "Epoch 621/1000, Loss: 0.0\n",
            "Epoch 622/1000, Loss: 0.0\n",
            "Epoch 623/1000, Loss: 0.0\n",
            "Epoch 624/1000, Loss: 0.0\n",
            "Epoch 625/1000, Loss: 0.0\n",
            "Epoch 626/1000, Loss: 0.0\n",
            "Epoch 627/1000, Loss: 0.0\n",
            "Epoch 628/1000, Loss: 0.0\n",
            "Epoch 629/1000, Loss: 0.0\n",
            "Epoch 630/1000, Loss: 0.0\n",
            "Epoch 631/1000, Loss: 0.0\n",
            "Epoch 632/1000, Loss: 0.0\n",
            "Epoch 633/1000, Loss: 0.0\n",
            "Epoch 634/1000, Loss: 0.0\n",
            "Epoch 635/1000, Loss: 0.0\n",
            "Epoch 636/1000, Loss: 0.0\n",
            "Epoch 637/1000, Loss: 0.0\n",
            "Epoch 638/1000, Loss: 0.0\n",
            "Epoch 639/1000, Loss: 0.0\n",
            "Epoch 640/1000, Loss: 0.0\n",
            "Epoch 641/1000, Loss: 0.0\n",
            "Epoch 642/1000, Loss: 0.0\n",
            "Epoch 643/1000, Loss: 0.0\n",
            "Epoch 644/1000, Loss: 0.0\n",
            "Epoch 645/1000, Loss: 0.0\n",
            "Epoch 646/1000, Loss: 0.0\n",
            "Epoch 647/1000, Loss: 0.0\n",
            "Epoch 648/1000, Loss: 0.0\n",
            "Epoch 649/1000, Loss: 0.0\n",
            "Epoch 650/1000, Loss: 0.0\n",
            "Epoch 651/1000, Loss: 0.0\n",
            "Epoch 652/1000, Loss: 0.0\n",
            "Epoch 653/1000, Loss: 0.0\n",
            "Epoch 654/1000, Loss: 0.0\n",
            "Epoch 655/1000, Loss: 0.0\n",
            "Epoch 656/1000, Loss: 0.0\n",
            "Epoch 657/1000, Loss: 0.0\n",
            "Epoch 658/1000, Loss: 0.0\n",
            "Epoch 659/1000, Loss: 0.0\n",
            "Epoch 660/1000, Loss: 0.0\n",
            "Epoch 661/1000, Loss: 0.0\n",
            "Epoch 662/1000, Loss: 0.0\n",
            "Epoch 663/1000, Loss: 0.0\n",
            "Epoch 664/1000, Loss: 0.0\n",
            "Epoch 665/1000, Loss: 0.0\n",
            "Epoch 666/1000, Loss: 0.0\n",
            "Epoch 667/1000, Loss: 0.0\n",
            "Epoch 668/1000, Loss: 0.0\n",
            "Epoch 669/1000, Loss: 0.0\n",
            "Epoch 670/1000, Loss: 0.0\n",
            "Epoch 671/1000, Loss: 0.0\n",
            "Epoch 672/1000, Loss: 0.0\n",
            "Epoch 673/1000, Loss: 0.0\n",
            "Epoch 674/1000, Loss: 0.0\n",
            "Epoch 675/1000, Loss: 0.0\n",
            "Epoch 676/1000, Loss: 0.0\n",
            "Epoch 677/1000, Loss: 0.0\n",
            "Epoch 678/1000, Loss: 0.0\n",
            "Epoch 679/1000, Loss: 0.0\n",
            "Epoch 680/1000, Loss: 0.0\n",
            "Epoch 681/1000, Loss: 0.0\n",
            "Epoch 682/1000, Loss: 0.0\n",
            "Epoch 683/1000, Loss: 0.0\n",
            "Epoch 684/1000, Loss: 0.0\n",
            "Epoch 685/1000, Loss: 0.0\n",
            "Epoch 686/1000, Loss: 0.0\n",
            "Epoch 687/1000, Loss: 0.0\n",
            "Epoch 688/1000, Loss: 0.0\n",
            "Epoch 689/1000, Loss: 0.0\n",
            "Epoch 690/1000, Loss: 0.0\n",
            "Epoch 691/1000, Loss: 0.0\n",
            "Epoch 692/1000, Loss: 0.0\n",
            "Epoch 693/1000, Loss: 0.0\n",
            "Epoch 694/1000, Loss: 0.0\n",
            "Epoch 695/1000, Loss: 0.0\n",
            "Epoch 696/1000, Loss: 0.0\n",
            "Epoch 697/1000, Loss: 0.0\n",
            "Epoch 698/1000, Loss: 0.0\n",
            "Epoch 699/1000, Loss: 0.0\n",
            "Epoch 700/1000, Loss: 0.0\n",
            "Epoch 701/1000, Loss: 0.0\n",
            "Epoch 702/1000, Loss: 0.0\n",
            "Epoch 703/1000, Loss: 0.0\n",
            "Epoch 704/1000, Loss: 0.0\n",
            "Epoch 705/1000, Loss: 0.0\n",
            "Epoch 706/1000, Loss: 0.0\n",
            "Epoch 707/1000, Loss: 0.0\n",
            "Epoch 708/1000, Loss: 0.0\n",
            "Epoch 709/1000, Loss: 0.0\n",
            "Epoch 710/1000, Loss: 0.0\n",
            "Epoch 711/1000, Loss: 0.0\n",
            "Epoch 712/1000, Loss: 0.0\n",
            "Epoch 713/1000, Loss: 0.0\n",
            "Epoch 714/1000, Loss: 0.0\n",
            "Epoch 715/1000, Loss: 0.0\n",
            "Epoch 716/1000, Loss: 0.0\n",
            "Epoch 717/1000, Loss: 0.0\n",
            "Epoch 718/1000, Loss: 0.0\n",
            "Epoch 719/1000, Loss: 0.0\n",
            "Epoch 720/1000, Loss: 0.0\n",
            "Epoch 721/1000, Loss: 0.0\n",
            "Epoch 722/1000, Loss: 0.0\n",
            "Epoch 723/1000, Loss: 0.0\n",
            "Epoch 724/1000, Loss: 0.0\n",
            "Epoch 725/1000, Loss: 0.0\n",
            "Epoch 726/1000, Loss: 0.0\n",
            "Epoch 727/1000, Loss: 0.0\n",
            "Epoch 728/1000, Loss: 0.0\n",
            "Epoch 729/1000, Loss: 0.0\n",
            "Epoch 730/1000, Loss: 0.0\n",
            "Epoch 731/1000, Loss: 0.0\n",
            "Epoch 732/1000, Loss: 0.0\n",
            "Epoch 733/1000, Loss: 0.0\n",
            "Epoch 734/1000, Loss: 0.0\n",
            "Epoch 735/1000, Loss: 0.0\n",
            "Epoch 736/1000, Loss: 0.0\n",
            "Epoch 737/1000, Loss: 0.0\n",
            "Epoch 738/1000, Loss: 0.0\n",
            "Epoch 739/1000, Loss: 0.0\n",
            "Epoch 740/1000, Loss: 0.0\n",
            "Epoch 741/1000, Loss: 0.0\n",
            "Epoch 742/1000, Loss: 0.0\n",
            "Epoch 743/1000, Loss: 0.0\n",
            "Epoch 744/1000, Loss: 0.0\n",
            "Epoch 745/1000, Loss: 0.0\n",
            "Epoch 746/1000, Loss: 0.0\n",
            "Epoch 747/1000, Loss: 0.0\n",
            "Epoch 748/1000, Loss: 0.0\n",
            "Epoch 749/1000, Loss: 0.0\n",
            "Epoch 750/1000, Loss: 0.0\n",
            "Epoch 751/1000, Loss: 0.0\n",
            "Epoch 752/1000, Loss: 0.0\n",
            "Epoch 753/1000, Loss: 0.0\n",
            "Epoch 754/1000, Loss: 0.0\n",
            "Epoch 755/1000, Loss: 0.0\n",
            "Epoch 756/1000, Loss: 0.0\n",
            "Epoch 757/1000, Loss: 0.0\n",
            "Epoch 758/1000, Loss: 0.0\n",
            "Epoch 759/1000, Loss: 0.0\n",
            "Epoch 760/1000, Loss: 0.0\n",
            "Epoch 761/1000, Loss: 0.0\n",
            "Epoch 762/1000, Loss: 0.0\n",
            "Epoch 763/1000, Loss: 0.0\n",
            "Epoch 764/1000, Loss: 0.0\n",
            "Epoch 765/1000, Loss: 0.0\n",
            "Epoch 766/1000, Loss: 0.0\n",
            "Epoch 767/1000, Loss: 0.0\n",
            "Epoch 768/1000, Loss: 0.0\n",
            "Epoch 769/1000, Loss: 0.0\n",
            "Epoch 770/1000, Loss: 0.0\n",
            "Epoch 771/1000, Loss: 0.0\n",
            "Epoch 772/1000, Loss: 0.0\n",
            "Epoch 773/1000, Loss: 0.0\n",
            "Epoch 774/1000, Loss: 0.0\n",
            "Epoch 775/1000, Loss: 0.0\n",
            "Epoch 776/1000, Loss: 0.0\n",
            "Epoch 777/1000, Loss: 0.0\n",
            "Epoch 778/1000, Loss: 0.0\n",
            "Epoch 779/1000, Loss: 0.0\n",
            "Epoch 780/1000, Loss: 0.0\n",
            "Epoch 781/1000, Loss: 0.0\n",
            "Epoch 782/1000, Loss: 0.0\n",
            "Epoch 783/1000, Loss: 0.0\n",
            "Epoch 784/1000, Loss: 0.0\n",
            "Epoch 785/1000, Loss: 0.0\n",
            "Epoch 786/1000, Loss: 0.0\n",
            "Epoch 787/1000, Loss: 0.0\n",
            "Epoch 788/1000, Loss: 0.0\n",
            "Epoch 789/1000, Loss: 0.0\n",
            "Epoch 790/1000, Loss: 0.0\n",
            "Epoch 791/1000, Loss: 0.0\n",
            "Epoch 792/1000, Loss: 0.0\n",
            "Epoch 793/1000, Loss: 0.0\n",
            "Epoch 794/1000, Loss: 0.0\n",
            "Epoch 795/1000, Loss: 0.0\n",
            "Epoch 796/1000, Loss: 0.0\n",
            "Epoch 797/1000, Loss: 0.0\n",
            "Epoch 798/1000, Loss: 0.0\n",
            "Epoch 799/1000, Loss: 0.0\n",
            "Epoch 800/1000, Loss: 0.0\n",
            "Epoch 801/1000, Loss: 0.0\n",
            "Epoch 802/1000, Loss: 0.0\n",
            "Epoch 803/1000, Loss: 0.0\n",
            "Epoch 804/1000, Loss: 0.0\n",
            "Epoch 805/1000, Loss: 0.0\n",
            "Epoch 806/1000, Loss: 0.0\n",
            "Epoch 807/1000, Loss: 0.0\n",
            "Epoch 808/1000, Loss: 0.0\n",
            "Epoch 809/1000, Loss: 0.0\n",
            "Epoch 810/1000, Loss: 0.0\n",
            "Epoch 811/1000, Loss: 0.0\n",
            "Epoch 812/1000, Loss: 0.0\n",
            "Epoch 813/1000, Loss: 0.0\n",
            "Epoch 814/1000, Loss: 0.0\n",
            "Epoch 815/1000, Loss: 0.0\n",
            "Epoch 816/1000, Loss: 0.0\n",
            "Epoch 817/1000, Loss: 0.0\n",
            "Epoch 818/1000, Loss: 0.0\n",
            "Epoch 819/1000, Loss: 0.0\n",
            "Epoch 820/1000, Loss: 0.0\n",
            "Epoch 821/1000, Loss: 0.0\n",
            "Epoch 822/1000, Loss: 0.0\n",
            "Epoch 823/1000, Loss: 0.0\n",
            "Epoch 824/1000, Loss: 0.0\n",
            "Epoch 825/1000, Loss: 0.0\n",
            "Epoch 826/1000, Loss: 0.0\n",
            "Epoch 827/1000, Loss: 0.0\n",
            "Epoch 828/1000, Loss: 0.0\n",
            "Epoch 829/1000, Loss: 0.0\n",
            "Epoch 830/1000, Loss: 0.0\n",
            "Epoch 831/1000, Loss: 0.0\n",
            "Epoch 832/1000, Loss: 0.0\n",
            "Epoch 833/1000, Loss: 0.0\n",
            "Epoch 834/1000, Loss: 0.0\n",
            "Epoch 835/1000, Loss: 0.0\n",
            "Epoch 836/1000, Loss: 0.0\n",
            "Epoch 837/1000, Loss: 0.0\n",
            "Epoch 838/1000, Loss: 0.0\n",
            "Epoch 839/1000, Loss: 0.0\n",
            "Epoch 840/1000, Loss: 0.0\n",
            "Epoch 841/1000, Loss: 0.0\n",
            "Epoch 842/1000, Loss: 0.0\n",
            "Epoch 843/1000, Loss: 0.0\n",
            "Epoch 844/1000, Loss: 0.0\n",
            "Epoch 845/1000, Loss: 0.0\n",
            "Epoch 846/1000, Loss: 0.0\n",
            "Epoch 847/1000, Loss: 0.0\n",
            "Epoch 848/1000, Loss: 0.0\n",
            "Epoch 849/1000, Loss: 0.0\n",
            "Epoch 850/1000, Loss: 0.0\n",
            "Epoch 851/1000, Loss: 0.0\n",
            "Epoch 852/1000, Loss: 0.0\n",
            "Epoch 853/1000, Loss: 0.0\n",
            "Epoch 854/1000, Loss: 0.0\n",
            "Epoch 855/1000, Loss: 0.0\n",
            "Epoch 856/1000, Loss: 0.0\n",
            "Epoch 857/1000, Loss: 0.0\n",
            "Epoch 858/1000, Loss: 0.0\n",
            "Epoch 859/1000, Loss: 0.0\n",
            "Epoch 860/1000, Loss: 0.0\n",
            "Epoch 861/1000, Loss: 0.0\n",
            "Epoch 862/1000, Loss: 0.0\n",
            "Epoch 863/1000, Loss: 0.0\n",
            "Epoch 864/1000, Loss: 0.0\n",
            "Epoch 865/1000, Loss: 0.0\n",
            "Epoch 866/1000, Loss: 0.0\n",
            "Epoch 867/1000, Loss: 0.0\n",
            "Epoch 868/1000, Loss: 0.0\n",
            "Epoch 869/1000, Loss: 0.0\n",
            "Epoch 870/1000, Loss: 0.0\n",
            "Epoch 871/1000, Loss: 0.0\n",
            "Epoch 872/1000, Loss: 0.0\n",
            "Epoch 873/1000, Loss: 0.0\n",
            "Epoch 874/1000, Loss: 0.0\n",
            "Epoch 875/1000, Loss: 0.0\n",
            "Epoch 876/1000, Loss: 0.0\n",
            "Epoch 877/1000, Loss: 0.0\n",
            "Epoch 878/1000, Loss: 0.0\n",
            "Epoch 879/1000, Loss: 0.0\n",
            "Epoch 880/1000, Loss: 0.0\n",
            "Epoch 881/1000, Loss: 0.0\n",
            "Epoch 882/1000, Loss: 0.0\n",
            "Epoch 883/1000, Loss: 0.0\n",
            "Epoch 884/1000, Loss: 0.0\n",
            "Epoch 885/1000, Loss: 0.0\n",
            "Epoch 886/1000, Loss: 0.0\n",
            "Epoch 887/1000, Loss: 0.0\n",
            "Epoch 888/1000, Loss: 0.0\n",
            "Epoch 889/1000, Loss: 0.0\n",
            "Epoch 890/1000, Loss: 0.0\n",
            "Epoch 891/1000, Loss: 0.0\n",
            "Epoch 892/1000, Loss: 0.0\n",
            "Epoch 893/1000, Loss: 0.0\n",
            "Epoch 894/1000, Loss: 0.0\n",
            "Epoch 895/1000, Loss: 0.0\n",
            "Epoch 896/1000, Loss: 0.0\n",
            "Epoch 897/1000, Loss: 0.0\n",
            "Epoch 898/1000, Loss: 0.0\n",
            "Epoch 899/1000, Loss: 0.0\n",
            "Epoch 900/1000, Loss: 0.0\n",
            "Epoch 901/1000, Loss: 0.0\n",
            "Epoch 902/1000, Loss: 0.0\n",
            "Epoch 903/1000, Loss: 0.0\n",
            "Epoch 904/1000, Loss: 0.0\n",
            "Epoch 905/1000, Loss: 0.0\n",
            "Epoch 906/1000, Loss: 0.0\n",
            "Epoch 907/1000, Loss: 0.0\n",
            "Epoch 908/1000, Loss: 0.0\n",
            "Epoch 909/1000, Loss: 0.0\n",
            "Epoch 910/1000, Loss: 0.0\n",
            "Epoch 911/1000, Loss: 0.0\n",
            "Epoch 912/1000, Loss: 0.0\n",
            "Epoch 913/1000, Loss: 0.0\n",
            "Epoch 914/1000, Loss: 0.0\n",
            "Epoch 915/1000, Loss: 0.0\n",
            "Epoch 916/1000, Loss: 0.0\n",
            "Epoch 917/1000, Loss: 0.0\n",
            "Epoch 918/1000, Loss: 0.0\n",
            "Epoch 919/1000, Loss: 0.0\n",
            "Epoch 920/1000, Loss: 0.0\n",
            "Epoch 921/1000, Loss: 0.0\n",
            "Epoch 922/1000, Loss: 0.0\n",
            "Epoch 923/1000, Loss: 0.0\n",
            "Epoch 924/1000, Loss: 0.0\n",
            "Epoch 925/1000, Loss: 0.0\n",
            "Epoch 926/1000, Loss: 0.0\n",
            "Epoch 927/1000, Loss: 0.0\n",
            "Epoch 928/1000, Loss: 0.0\n",
            "Epoch 929/1000, Loss: 0.0\n",
            "Epoch 930/1000, Loss: 0.0\n",
            "Epoch 931/1000, Loss: 0.0\n",
            "Epoch 932/1000, Loss: 0.0\n",
            "Epoch 933/1000, Loss: 0.0\n",
            "Epoch 934/1000, Loss: 0.0\n",
            "Epoch 935/1000, Loss: 0.0\n",
            "Epoch 936/1000, Loss: 0.0\n",
            "Epoch 937/1000, Loss: 0.0\n",
            "Epoch 938/1000, Loss: 0.0\n",
            "Epoch 939/1000, Loss: 0.0\n",
            "Epoch 940/1000, Loss: 0.0\n",
            "Epoch 941/1000, Loss: 0.0\n",
            "Epoch 942/1000, Loss: 0.0\n",
            "Epoch 943/1000, Loss: 0.0\n",
            "Epoch 944/1000, Loss: 0.0\n",
            "Epoch 945/1000, Loss: 0.0\n",
            "Epoch 946/1000, Loss: 0.0\n",
            "Epoch 947/1000, Loss: 0.0\n",
            "Epoch 948/1000, Loss: 0.0\n",
            "Epoch 949/1000, Loss: 0.0\n",
            "Epoch 950/1000, Loss: 0.0\n",
            "Epoch 951/1000, Loss: 0.0\n",
            "Epoch 952/1000, Loss: 0.0\n",
            "Epoch 953/1000, Loss: 0.0\n",
            "Epoch 954/1000, Loss: 0.0\n",
            "Epoch 955/1000, Loss: 0.0\n",
            "Epoch 956/1000, Loss: 0.0\n",
            "Epoch 957/1000, Loss: 0.0\n",
            "Epoch 958/1000, Loss: 0.0\n",
            "Epoch 959/1000, Loss: 0.0\n",
            "Epoch 960/1000, Loss: 0.0\n",
            "Epoch 961/1000, Loss: 0.0\n",
            "Epoch 962/1000, Loss: 0.0\n",
            "Epoch 963/1000, Loss: 0.0\n",
            "Epoch 964/1000, Loss: 0.0\n",
            "Epoch 965/1000, Loss: 0.0\n",
            "Epoch 966/1000, Loss: 0.0\n",
            "Epoch 967/1000, Loss: 0.0\n",
            "Epoch 968/1000, Loss: 0.0\n",
            "Epoch 969/1000, Loss: 0.0\n",
            "Epoch 970/1000, Loss: 0.0\n",
            "Epoch 971/1000, Loss: 0.0\n",
            "Epoch 972/1000, Loss: 0.0\n",
            "Epoch 973/1000, Loss: 0.0\n",
            "Epoch 974/1000, Loss: 0.0\n",
            "Epoch 975/1000, Loss: 0.0\n",
            "Epoch 976/1000, Loss: 0.0\n",
            "Epoch 977/1000, Loss: 0.0\n",
            "Epoch 978/1000, Loss: 0.0\n",
            "Epoch 979/1000, Loss: 0.0\n",
            "Epoch 980/1000, Loss: 0.0\n",
            "Epoch 981/1000, Loss: 0.0\n",
            "Epoch 982/1000, Loss: 0.0\n",
            "Epoch 983/1000, Loss: 0.0\n",
            "Epoch 984/1000, Loss: 0.0\n",
            "Epoch 985/1000, Loss: 0.0\n",
            "Epoch 986/1000, Loss: 0.0\n",
            "Epoch 987/1000, Loss: 0.0\n",
            "Epoch 988/1000, Loss: 0.0\n",
            "Epoch 989/1000, Loss: 0.0\n",
            "Epoch 990/1000, Loss: 0.0\n",
            "Epoch 991/1000, Loss: 0.0\n",
            "Epoch 992/1000, Loss: 0.0\n",
            "Epoch 993/1000, Loss: 0.0\n",
            "Epoch 994/1000, Loss: 0.0\n",
            "Epoch 995/1000, Loss: 0.0\n",
            "Epoch 996/1000, Loss: 0.0\n",
            "Epoch 997/1000, Loss: 0.0\n",
            "Epoch 998/1000, Loss: 0.0\n",
            "Epoch 999/1000, Loss: 0.0\n",
            "Epoch 1000/1000, Loss: 0.0\n",
            "Текст: Сегодня в новостях рассказывают о любви и семье. Также упоминается важность дружбы и денег.\n",
            "Классификация: ['семья', 'дружба', 'деньги']\n"
          ]
        }
      ]
    }
  ]
}